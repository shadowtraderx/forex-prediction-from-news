{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5184fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import traceback\n",
    "import os.path\n",
    "import csv\n",
    "from csv import writer\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487c3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeDriver():\n",
    "    \"\"\"\n",
    "    Initializes the web browser driver.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    driver\n",
    "    \"\"\"\n",
    "    \n",
    "    chrome_path = \"chromedriver\"\n",
    "\n",
    "    driver=webdriver.Chrome(chrome_path)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    Replaces non-alphanumeric characters in text with a space\n",
    "    \"\"\"\n",
    "    import re\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def scrape_article_text(driver):\n",
    "                    \n",
    "    try:\n",
    "        #text = driver.find_element_by_xpath('/html/body/div[1]/div/div[4]/div[1]/article/div[1]').text\n",
    "\n",
    "        params =  driver.find_elements_by_xpath('/html/body/div[1]/div/div[4]/div[1]/article/p')\n",
    "\n",
    "        for p in params:\n",
    "            print(p.text)\n",
    "\n",
    "    except Exception as ex:\n",
    "        try:\n",
    "            # text = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[2]/div/div[1]/article/div/div/div/div\").text\n",
    "            params =  driver.find_elements_by_xpath('/html/body/div[1]/div/div[2]/div/div[1]/article/p')\n",
    "\n",
    "            for p in params:\n",
    "                print(p.text)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "\n",
    "def scrape_timestamp(driver):\n",
    "    \n",
    "    timestamp = None\n",
    "    \n",
    "    try:\n",
    "        timestamp = driver.find_element_by_xpath(\"/html/head/meta[@name='article:published_time']\").get_attribute('content')\n",
    "\n",
    "    except Exception as ex:\n",
    "        try:\n",
    "            timestamp = driver.find_element_by_xpath(\"/html/head/meta[@property='og:article:published_time']\").get_attribute('content')\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "    \n",
    "    return timestamp\n",
    "\n",
    "def scrape_category(driver):\n",
    "    \n",
    "    category = None\n",
    "    \n",
    "    try:\n",
    "        category = driver.find_element_by_xpath(\"/html/head/meta[@name='analyticsAttributes.topicChannel']\").get_attribute('content')\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "    \n",
    "    return category\n",
    "\n",
    "def scrape_subcategory(driver):\n",
    "    \n",
    "    subcategory = None\n",
    "    \n",
    "    try:\n",
    "        subcategory = driver.find_element_by_xpath(\"/html/head/meta[@name='analyticsAttributes.topicSubChannel']\").get_attribute('content')\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "    \n",
    "    return subcategory\n",
    "\n",
    "def scrape_page(link, driver):\n",
    "    \n",
    "    driver.get(link)\n",
    "    \n",
    "    article_list = driver.find_elements_by_tag_name('article')\n",
    "    \n",
    "    hrefs = []\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for article in article_list:\n",
    "        try:\n",
    "            title = cleanText(article.find_element_by_xpath('./div[2]/a/h3').text)\n",
    "            href = article.find_element_by_xpath('./div[2]/a').get_attribute('href')\n",
    "            desc = cleanText(article.find_element_by_xpath('./div[2]/p').text)\n",
    "            date = article.find_element_by_xpath('./div[2]/time/span').text\n",
    "            hrefs.append(href)\n",
    "            \n",
    "            data_list.append([href,title,desc,date])\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "            \n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.columns = ['href','title','desc','date']\n",
    "    df['timestamp'] = pd.Series(index=df.index, dtype='object')\n",
    "    df['category'] = pd.Series(index=df.index, dtype='object')\n",
    "    df['sub_category'] = pd.Series(index=df.index, dtype='object')\n",
    "    \n",
    "    i = 0\n",
    "    for href in hrefs:\n",
    "        i+=1\n",
    "        print(str(i))\n",
    "        driver.get(href)\n",
    "        #time.sleep(1)\n",
    "        \n",
    "        idx = df.index[df['href'] == href].tolist()[0]\n",
    "        \n",
    "        timestamp = scrape_timestamp(driver)\n",
    "        df['timestamp'].iloc[idx] = timestamp\n",
    "        \n",
    "        category = scrape_category(driver)\n",
    "        df['category'].iloc[idx] = category\n",
    "        \n",
    "        sub_category = scrape_subcategory(driver)\n",
    "        df['sub_category'].iloc[idx] = sub_category\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e47d297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 769\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"./div[2]/time/span\"}\n",
      "  (Session info: chrome=90.0.4430.212)\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"./div[2]/p\"}\n",
      "  (Session info: chrome=90.0.4430.212)\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"./div[2]/p\"}\n",
      "  (Session info: chrome=90.0.4430.212)\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "                                                href  \\\n",
      "0  https://www.reuters.com/article/us-tech-ces-au...   \n",
      "1  https://www.reuters.com/article/us-japan-boj-i...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Automakers not Silicon Valley lead in driverle...   \n",
      "1  Japan central bank turns activist investor to ...   \n",
      "\n",
      "                                                desc         date  \\\n",
      "0  Automakers not technology companies are in the...  JAN 05 2016   \n",
      "1  Japan s central bank which dominates the domes...  JAN 03 2016   \n",
      "\n",
      "              timestamp  category      sub_category  \n",
      "0  2016-01-05T18:10:38Z  Business  Media & Telecoms  \n",
      "1  2016-01-03T21:10:27Z  Business     Business Home  \n",
      "Page: 770\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"./div[2]/time/span\"}\n",
      "  (Session info: chrome=90.0.4430.212)\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"./div[2]/p\"}\n",
      "  (Session info: chrome=90.0.4430.212)\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"./div[2]/p\"}\n",
      "  (Session info: chrome=90.0.4430.212)\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "                                                href  \\\n",
      "0  https://www.reuters.com/article/us-japan-south...   \n",
      "1  https://www.reuters.com/article/us-pep-boys-ma...   \n",
      "\n",
      "                                               title  \\\n",
      "0  South Korea Japan agree to irreversibly end co...   \n",
      "1  Carl Icahn trumps Bridgestone s offer for Pep ...   \n",
      "\n",
      "                                                desc         date  \\\n",
      "0  South Korea and Japan reached a landmark agree...  DEC 28 2015   \n",
      "1  Activist investor Carl Icahn trumped Japanese ...  DEC 28 2015   \n",
      "\n",
      "              timestamp  category      sub_category  \n",
      "0  2015-12-29T00:24:46Z   Markets  Emerging Markets  \n",
      "1  2015-12-28T22:02:36Z  Business             Autos  \n"
     ]
    }
   ],
   "source": [
    "driver = initializeDriver()\n",
    "\n",
    "for i in range(769,771):\n",
    "    print(\"Page: \"+str(i))\n",
    "    link = 'https://www.reuters.com/news/archive/japan?view=page&page='+str(i)+'&pageSize=10'\n",
    "    df = scrape_page(link, driver)\n",
    "    print(df.head(2))\n",
    "    df.to_csv(\"scraped_news.csv\",mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16475fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
